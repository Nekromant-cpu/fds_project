{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9mvAJjk1LKu"
      },
      "outputs": [],
      "source": [
        "!git clone https://GITHUBTOKEN@github.com/Nekromant-cpu/fds_project.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OxGbpCew1Ydj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "IfW0iczezrEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 512  # Universal Sentence Encoder produces 512-dimensional embeddings\n",
        "batch_size = 16\n",
        "num_classes = 29"
      ],
      "metadata": {
        "id": "lek0w01VMC0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import nltk\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# File paths\n",
        "csv_file_path_train = './fds_project/data/full_data/book_descriptions_train_balanced.csv'\n",
        "csv_file_path_test = './fds_project/data/full_data/book_descriptions_test_balanced.csv'\n",
        "images_folder = './fds_project/data/images/'\n",
        "\n",
        "\n",
        "def clean_text_column(text: str) -> str:\n",
        "    # Replace digits and punctuation with spaces\n",
        "    text = text.translate(str.maketrans(string.digits, \" \" * len(string.digits)))\n",
        "    text = text.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
        "    # Collapse multiple spaces\n",
        "    text = \" \".join(text.split())\n",
        "    # Tokenize and remove stopwords\n",
        "    words = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    cleaned_text = \" \".join([word for word in words if word not in stop_words])\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "# Read and process training data\n",
        "def load_data(csv_file_path, images_folder):\n",
        "    image_names = []\n",
        "    descriptions = []\n",
        "    category_ids = []\n",
        "\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    df.dropna(subset=[\"description\"], inplace=True)\n",
        "    df.dropna(subset=[\"title\"], inplace=True)\n",
        "\n",
        "\n",
        "    df['description_cleaned'] = df['description'].apply(clean_text_column)\n",
        "    df['title_cleaned'] = df['title'].apply(clean_text_column)\n",
        "\n",
        "    df['description'] = df['title_cleaned'] + \" \" + df['description_cleaned']\n",
        "\n",
        "\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        img_name = row['img_name']\n",
        "        description = row['description']\n",
        "\n",
        "        tokens = word_tokenize(description)\n",
        "        truncated_description = \" \".join(tokens[:500])\n",
        "\n",
        "        category_id = row['category_id']  # Category as string\n",
        "\n",
        "        # Convert category_id from str to int\n",
        "        try:\n",
        "            category_id = int(category_id)\n",
        "        except ValueError:\n",
        "            print(f\"Invalid category ID {category_id}. Skipping entry.\")\n",
        "            continue\n",
        "\n",
        "        # Append the data to respective lists\n",
        "        image_names.append(img_name)\n",
        "        descriptions.append(truncated_description)\n",
        "        category_ids.append(category_id)\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    images = np.array(image_names)\n",
        "    descriptions = np.array(descriptions, dtype=object)  # Keep descriptions as strings\n",
        "    category_ids = np.array(category_ids)\n",
        "\n",
        "    return images, descriptions, category_ids\n",
        "\n",
        "# Load training and testing data\n",
        "image_names_train, descriptions_train, category_ids_train = load_data(csv_file_path_train, images_folder)\n",
        "image_names_test, descriptions_test, category_ids_test = load_data(csv_file_path_test, images_folder)\n",
        "\n",
        "# Output shapes and types to verify\n",
        "print(\"Training Data:\")\n",
        "print(\"Images shape:\", image_names_train.shape)\n",
        "print(\"Descriptions shape:\", descriptions_train.shape)\n",
        "print(\"Category IDs shape:\", category_ids_train.shape)\n",
        "print(\"Category IDs dtype:\", category_ids_train.dtype)\n",
        "\n",
        "print(\"\\nTesting Data:\")\n",
        "print(\"Images shape:\", image_names_test.shape)\n",
        "print(\"Descriptions shape:\", descriptions_test.shape)\n",
        "print(\"Category IDs shape:\", category_ids_test.shape)\n",
        "print(\"Category IDs dtype:\", category_ids_test.dtype)"
      ],
      "metadata": {
        "id": "uhQchN5R1EYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### execute the following block only if emebeddings are not already precomputed"
      ],
      "metadata": {
        "id": "4Z_9gpu_zw9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Load USE model\n",
        "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "\n",
        "def text_to_use_embedding(sentences, use_model):\n",
        "    embeddings = []\n",
        "    for sentence in tqdm(sentences, desc=\"Processing sentences\", unit=\"sentence\"):\n",
        "        if not sentence or len(sentence.strip()) == 0:\n",
        "            embeddings.append(np.zeros((embedding_dim,)))  # Empty description placeholder\n",
        "            continue\n",
        "\n",
        "        # Generate USE embeddings\n",
        "        sentence_embedding = use_model([sentence]).numpy().squeeze(0)\n",
        "\n",
        "        embeddings.append(sentence_embedding)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "1PQ-Pzba8XK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load embeddings"
      ],
      "metadata": {
        "id": "hfl3FrSdz5xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use only for evaluation\n",
        "embedding = \"USE/USE_cleaned\"\n",
        "\n",
        "try:\n",
        "    text_embeddings_train = np.load(f\"/content/drive/MyDrive/{embedding}/text_embeddings_train.npy\")\n",
        "    text_embeddings_test = np.load(f\"/content/drive/MyDrive/{embedding}/text_embeddings_test.npy\")\n",
        "except OSError:\n",
        "    # Precompute and save USE embeddings\n",
        "    text_embeddings_train = text_to_use_embedding(descriptions_train, use_model)\n",
        "    np.save(\"/content/drive/MyDrive/USE/text_embeddings_train.npy\", text_embeddings_train)\n",
        "\n",
        "    text_embeddings_test = text_to_use_embedding(descriptions_test, use_model)\n",
        "    np.save(\"/content/drive/MyDrive/USE/text_embeddings_test.npy\", text_embeddings_test)\n"
      ],
      "metadata": {
        "id": "efPoCjML1iQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create training, validation and testing dataset"
      ],
      "metadata": {
        "id": "Ox7ii7LZ0B08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "def data_generator(img_names, descriptions_embeddings, category_ids, images_folder, batch_size=16, shuffle=True):\n",
        "    def parse_function(img_name, embedding, category_id):\n",
        "        # Load and preprocess the image\n",
        "        image_path = tf.strings.join([images_folder, img_name])\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        image = tf.image.resize(image, [224, 224])  # Resize\n",
        "        image = tf.cast(image, tf.float32) / 255.0  # Normalize\n",
        "\n",
        "        embedding = tf.cast(embedding, tf.float32) # float64 to float32\n",
        "\n",
        "        return (image, embedding), tf.one_hot(category_id, num_classes)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_names, descriptions_embeddings, category_ids))\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = (\n",
        "            dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .shuffle(buffer_size=1000)\n",
        "            .batch(batch_size)\n",
        "            .prefetch(tf.data.AUTOTUNE)\n",
        "        )\n",
        "    else:\n",
        "        dataset = (\n",
        "            dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .batch(batch_size)\n",
        "            .prefetch(tf.data.AUTOTUNE)\n",
        "        )\n",
        "    return dataset\n",
        "\n",
        "train_img_names, val_img_names, train_embeddings, val_embeddings, train_category_ids, val_category_ids = train_test_split(\n",
        "    image_names_train, text_embeddings_train, category_ids_train, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = data_generator(\n",
        "    train_img_names, train_embeddings, train_category_ids, images_folder, batch_size\n",
        ")\n",
        "validation_dataset = data_generator(\n",
        "    val_img_names, val_embeddings, val_category_ids, images_folder, batch_size\n",
        ")\n",
        "test_dataset = data_generator(\n",
        "    image_names_test, text_embeddings_test, category_ids_test, images_folder, batch_size\n",
        ")\n",
        "\n",
        "# only for plotting after training\n",
        "test_dataset_without_shuffle = data_generator(\n",
        "    image_names_test, text_embeddings_test, category_ids_test, images_folder, batch_size, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "7X6hklID9DVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "A_cCKFqSzmcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, concatenate\n",
        "from tensorflow.keras.applications import ResNet50, MobileNetV2\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/fds/models/best_model.keras', save_best_only=True, save_weights_only=False)\n",
        "]\n",
        "\n",
        "\n",
        "# Image Feature Extractor using ImageNet v2\n",
        "def build_image_model():\n",
        "    #base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3)) # (128, 128, 3)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    return base_model.input, x\n",
        "\n",
        "# Joint Model\n",
        "def build_joint_model(embedding_dim, num_classes):\n",
        "    # Image Model\n",
        "    image_input, image_features = build_image_model()\n",
        "\n",
        "    # Text Model\n",
        "    text_input = Input(shape=(embedding_dim,), name=\"text_input\")  # Precomputed text embeddings\n",
        "    text_features = Dense(128, activation='relu')(text_input)\n",
        "\n",
        "    # Combine Features\n",
        "    combined = concatenate([image_features, text_features])\n",
        "    combined = Dense(256, activation='relu')(combined) # 128\n",
        "    combined = Dense(128, activation='relu')(combined) # 64\n",
        "\n",
        "    # Output\n",
        "    output = Dense(num_classes, activation='softmax', name=\"output\")(combined)\n",
        "\n",
        "    # Model\n",
        "    model = tf.keras.Model(inputs=[image_input, text_input], outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build the joint model\n",
        "model = build_joint_model(embedding_dim, num_classes)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5BKhDSp41m8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model after training\n",
        "model.save('/content/drive/MyDrive/fds/models/model1.keras')"
      ],
      "metadata": {
        "id": "qptoIiWzqX5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue training if needed"
      ],
      "metadata": {
        "id": "yPowh7t83CbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the previously saved model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/fds/models/best_model.keras')\n",
        "\n",
        "continue_epoch = 3\n",
        "\n",
        "# Continue training from the 3rd epoch\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        "    initial_epoch=continue_epoch\n",
        ")\n"
      ],
      "metadata": {
        "id": "DF-PNB-K3D1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "y8Lz-o45t_1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "load some model"
      ],
      "metadata": {
        "id": "cMgVhL5ZuD9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/fds/models_cleaned/ResNet+USE_16_10val.keras')"
      ],
      "metadata": {
        "id": "eNGvcarfY7ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test_loss, test_accuracy = loaded_model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Get true labels and predicted labels\n",
        "y_true = np.concatenate([y for x, y in test_dataset_without_shuffle], axis=0)  # True labels\n",
        "y_pred_probs = loaded_model.predict(test_dataset_without_shuffle)             # Predicted probabilities\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)                      # Predicted classes\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    y_true=np.argmax(y_true, axis=1),  # Convert one-hot to integer labels if needed\n",
        "    y_pred=y_pred,\n",
        "    average='weighted'                # Use 'weighted' for class imbalance\n",
        ")\n",
        "\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(np.argmax(y_true, axis=1), y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z-5hu2F5uGQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_mapping = {\n",
        "    0: \"Arts & Photography\",\n",
        "    1: \"Biographies & Memoirs\",\n",
        "    2: \"Business & Money\",\n",
        "    3: \"Children's Books\",\n",
        "    4: \"Comics & Graphic Novels\",\n",
        "    5: \"Computers & Technology\",\n",
        "    6: \"Cookbooks, Food & Wine\",\n",
        "    7: \"Crafts, Hobbies & Home\",\n",
        "    8: \"Christian Books & Bibles\",\n",
        "    9: \"Engineering & Transportation\",\n",
        "    10: \"Health, Fitness & Dieting\",\n",
        "    11: \"History\",\n",
        "    12: \"Humor & Entertainment\",\n",
        "    13: \"Law\",\n",
        "    14: \"Literature & Fiction\",\n",
        "    15: \"Medical Books\",\n",
        "    16: \"Mystery, Thriller & Suspense\",\n",
        "    17: \"Parenting & Relationships\",\n",
        "    18: \"Politics & Social Sciences\",\n",
        "    19: \"Reference\",\n",
        "    20: \"Religion & Spirituality\",\n",
        "    21: \"Romance\",\n",
        "    22: \"Science & Math\",\n",
        "    23: \"Science Fiction & Fantasy\",\n",
        "    24: \"Self-Help\",\n",
        "    25: \"Sports & Outdoors\",\n",
        "    26: \"Teen & Young Adult\",\n",
        "    27: \"Test Preparation\",\n",
        "    28: \"Travel\",\n",
        "}"
      ],
      "metadata": {
        "id": "uxfUfQc-rV-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-k (5) accuracy"
      ],
      "metadata": {
        "id": "PW8w4rIvuH2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
        "\n",
        "def calculate_top_k_accuracy(model, dataset, k=5):\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for batch in dataset:\n",
        "        # Unpack data\n",
        "        x, y_true = batch\n",
        "\n",
        "        # Predict probabilities\n",
        "        y_pred = model.predict(x)\n",
        "\n",
        "        # Get the indices of the top k predictions\n",
        "        top_k_preds = np.argsort(y_pred, axis=-1)[:, -k:]\n",
        "\n",
        "        # Compare with true labels\n",
        "        for true, top_k in zip(np.argmax(y_true, axis=-1), top_k_preds):\n",
        "            if true in top_k:\n",
        "                correct_predictions += 1\n",
        "\n",
        "        total_samples += y_true.shape[0]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    top_k_accuracy = correct_predictions / total_samples\n",
        "    return top_k_accuracy\n",
        "\n",
        "# Example usage:\n",
        "k=5\n",
        "top_k_acc = calculate_top_k_accuracy(loaded_model, test_dataset, k=k)\n",
        "print(f\"Top-k Accuracy with k={k}: {top_k_acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "Bl8q1FDMkjPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "top-k accuracy by class"
      ],
      "metadata": {
        "id": "BMCYQFJdudDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_top_k_accuracy_by_class(model, dataset, k=5, num_classes=29):\n",
        "    class_correct = np.zeros(num_classes)\n",
        "    class_total = np.zeros(num_classes)\n",
        "\n",
        "    for batch in dataset:\n",
        "        # Unpack data\n",
        "        x, y_true = batch\n",
        "\n",
        "        # Predict probabilities\n",
        "        y_pred = model.predict(x)\n",
        "\n",
        "        # Get the indices of the top k predictions\n",
        "        top_k_preds = np.argsort(y_pred, axis=-1)[:, -k:]\n",
        "\n",
        "        # Compare with true labels\n",
        "        for true, top_k in zip(np.argmax(y_true, axis=-1), top_k_preds):\n",
        "            class_total[true] += 1\n",
        "            if true in top_k:\n",
        "                class_correct[true] += 1\n",
        "\n",
        "    # Calculate top-k accuracy for each class\n",
        "    class_accuracy = class_correct / class_total\n",
        "\n",
        "    # Sort the classes by accuracy in descending order\n",
        "    sorted_indices = np.argsort(class_accuracy)[::-1]\n",
        "\n",
        "    # Print sorted top-k accuracy with category names\n",
        "    print(f\"Top-K Accuracy by Class with k={k} (sorted):\\n\")\n",
        "    for class_id in sorted_indices:\n",
        "        accuracy = class_accuracy[class_id]\n",
        "        category_name = category_mapping[class_id]\n",
        "        print(f\"{category_name}: {accuracy:.2f}\")\n",
        "\n",
        "# Example usage:\n",
        "top_k_acc_by_class = calculate_top_k_accuracy_by_class(loaded_model, test_dataset, k=5, num_classes=29)\n"
      ],
      "metadata": {
        "id": "BJ-tRgrNpdna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "top-1 accuracy sorted by class"
      ],
      "metadata": {
        "id": "cReAKSh0Lj6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_top_k_accuracy_by_class(model, dataset, k=5, num_classes=29):\n",
        "    class_correct = np.zeros(num_classes)\n",
        "    class_total = np.zeros(num_classes)\n",
        "\n",
        "    for batch in dataset:\n",
        "        # Unpack data\n",
        "        x, y_true = batch\n",
        "\n",
        "        # Predict probabilities\n",
        "        y_pred = model.predict(x)\n",
        "\n",
        "        # Get the indices of the top k predictions\n",
        "        top_k_preds = np.argsort(y_pred, axis=-1)[:, -k:]\n",
        "\n",
        "        # Compare with true labels\n",
        "        for true, top_k in zip(np.argmax(y_true, axis=-1), top_k_preds):\n",
        "            class_total[true] += 1\n",
        "            if true in top_k:\n",
        "                class_correct[true] += 1\n",
        "\n",
        "    # Calculate top-k accuracy for each class\n",
        "    class_accuracy = class_correct / class_total\n",
        "\n",
        "    # Sort the classes by accuracy in descending order\n",
        "    sorted_indices = np.argsort(class_accuracy)[::-1]\n",
        "\n",
        "    # Print sorted top-k accuracy with category names\n",
        "    print(f\"Top-K Accuracy by Class with k={k} (sorted):\\n\")\n",
        "    for class_id in sorted_indices:\n",
        "        accuracy = class_accuracy[class_id]\n",
        "        category_name = category_mapping[class_id]\n",
        "        print(f\"{category_name}: {accuracy:.2f}\")\n",
        "\n",
        "# Example usage:\n",
        "top_k_acc_by_class = calculate_top_k_accuracy_by_class(loaded_model, test_dataset, k=1, num_classes=29)\n"
      ],
      "metadata": {
        "id": "0MPIbvbL1vK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example prediction"
      ],
      "metadata": {
        "id": "yUdPKq2JuhTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=50):\n",
        "    return textwrap.fill(text, width=width)\n",
        "\n",
        "# Randomly select 8 samples from the dataset\n",
        "def random_sample_from_dataset(dataset, n=8):\n",
        "    dataset = list(dataset)  # Convert to list for sampling\n",
        "    chosen_indices = random.sample(range(len(dataset)), n)\n",
        "    return [dataset[i] for i in chosen_indices], chosen_indices\n",
        "    #return random.sample(dataset, n)\n",
        "\n",
        "def get_original_description(sample_index):\n",
        "    # Fetch and truncate description\n",
        "    description_text = descriptions_test[sample_index * batch_size][:80] + \"...\"\n",
        "    return description_text\n",
        "\n",
        "def visualize_predictions(samples, sample_idx, model, category_mapping):\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 12))\n",
        "    axes = axes.flatten()\n",
        "    fig.tight_layout(pad=5.0)\n",
        "\n",
        "    for i, (sample, sample_index) in enumerate(zip(samples, sample_idx)):\n",
        "        data, label = sample\n",
        "        image_batch, description_batch = data[0], data[1]\n",
        "        true_label = np.argmax(label)\n",
        "\n",
        "        # Extract a single sample from the batch\n",
        "        image = image_batch[0]\n",
        "        description = description_batch[0]\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = model.predict((image[None, ...], description[None, ...]))\n",
        "        top_k = np.argsort(predictions[0])[-5:][::-1]\n",
        "        top_k_probs = predictions[0][top_k]\n",
        "        top_k_labels = [category_mapping[idx] for idx in top_k]\n",
        "\n",
        "        description_text = get_original_description(sample_index)\n",
        "\n",
        "        # Display the image\n",
        "        ax = axes[i]\n",
        "        ax.imshow(image.numpy())\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Create a readable caption\n",
        "        true_category = f\"True: {category_mapping[true_label]}\"\n",
        "        predicted_category = \"Pred: \" + \", \".join(\n",
        "            [f\"{label} ({prob:.2f})\" for label, prob in zip(top_k_labels, top_k_probs)]\n",
        "        )\n",
        "\n",
        "        caption = f\"{true_category}\\n{wrap_text(predicted_category, width=50)}\\nDesc: {wrap_text(description_text, width=50)}\"\n",
        "\n",
        "        # Add caption\n",
        "        ax.set_title(caption, fontsize=10, loc='center', wrap=True)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage\n",
        "samples, sample_idx = random_sample_from_dataset(test_dataset_without_shuffle)\n",
        "visualize_predictions(samples, sample_idx, loaded_model, category_mapping)\n"
      ],
      "metadata": {
        "id": "XK63oU9LYCus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Own book"
      ],
      "metadata": {
        "id": "M5CURMT_TaGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "description = \"\"\"\n",
        "\n",
        "\n",
        "Your faithful companion throughout the grades with all relevant topics in mathematics.\n",
        "\n",
        "    128 pages\n",
        "    All topics from 5th to 10th grade\n",
        "    for grammar schools, comprehensive schools, secondary schools and intermediate schools\n",
        "    101 learning videos by Daniel Jung\n",
        "    21 tasks at exam level incl. solutions\n",
        "\n",
        "\n",
        "In this learning booklet for lower secondary level, you will find explanations and sample exercises on all relevant topics from 5th to 10th grade for grammar schools, comprehensive schools, lower secondary schools and intermediate secondary schools.\n",
        "\n",
        "Regardless of whether you simply want to consolidate the current material or systematically prepare for the upcoming final exam (ZAP or MSA), you will find what you are looking for here! Seemingly complex contexts are presented in a language that you can fully understand. You will also find all the formulas, sketches and anything else you need to understand the subject matter. As a special bonus, this booklet contains a short introduction to spreadsheets (Excel).\n",
        "\n",
        "The 5th-10th grade booklet will be your faithful companion throughout the grades, helping you to brush up on important topics and prepare you optimally for the mathematical content.\n",
        "\n",
        "All topics are divided into clear chapters that will guide you in your preparation:\n",
        "\n",
        "    Basics\n",
        "    Fractions\n",
        "    Negative numbers\n",
        "    Multiplication/factorization (factoring out)\n",
        "    Terms and equations/fractional equations/inequalities\n",
        "    Assignments and dr\"\"\"\n",
        "\n",
        "description_embedding = text_to_use_embedding([description], use_model)\n",
        "\n",
        "image = tf.io.read_file(\"71mrv+fvwwL._SL1500_.jpg\")\n",
        "image = tf.image.decode_jpeg(image, channels=3)\n",
        "image = tf.image.resize(image, [224, 224])  # Resize\n",
        "image = tf.cast(image, tf.float32) / 255.0  # Normalize\n",
        "image = tf.expand_dims(image, axis=0)  # Add batch dimension\n",
        "\n",
        "# Make predictions\n",
        "predictions = loaded_model.predict((image, description_embedding))\n",
        "top_k = np.argsort(predictions[0])[-5:][::-1]\n",
        "top_k_probs = predictions[0][top_k]\n",
        "top_k_labels = [category_mapping[idx] for idx in top_k]\n",
        "\n",
        "# Create a readable caption\n",
        "predicted_category = \"Pred: \" + \", \".join(\n",
        "    [f\"{label} ({prob:.2f})\" for label, prob in zip(top_k_labels, top_k_probs)]\n",
        ")\n",
        "\n",
        "print(predicted_category)"
      ],
      "metadata": {
        "id": "y2Lv49iMqTMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}